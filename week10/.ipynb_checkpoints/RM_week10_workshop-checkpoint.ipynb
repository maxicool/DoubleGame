{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28aa22b2",
   "metadata": {},
   "source": [
    "# Facets of Mathematics - Week 10\n",
    "# Regression Modelling\n",
    "\n",
    "### Aims\n",
    "\n",
    "By the end of this notebook you \n",
    "\n",
    "* should have understood how categorical variables can be incorporated in regression models\n",
    "* will be able to fit multiple linear models and logistic models to data sets\n",
    "* have worked with a real data set\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Categorical Variables in Regression Models\n",
    "* Multiple Linear Regression\n",
    "* Logistic Regression\n",
    "\n",
    "### References\n",
    "\n",
    "**Montgomery, Peck, and Vining:** D. C, Montgomery, E. A. Peck, and G. G. Vining, \"Introduction to Linear Regression Analysis\", 5th Edition, Wiley Series in Probability and Statistics, 2012\n",
    "\n",
    "* Multiple Linear Regression: Chapter 3, Sections 3.1 and 3.2\n",
    "\n",
    "**Davison:** A. C. Davison, \"Statistical Models\", Cambridge Series in Statistical and Probabilistic Mathematics, 2008\n",
    "\n",
    "* Linear Regression: Chapter 8\n",
    "* Generalised Linear Models: Section 10.3\n",
    "* Logistic Regression: Section 10.4\n",
    "\n",
    "**Draper, Smith:** N. R. Draper, H. Smith, \"Applied Regression Analysis\", 3rd Edition, Wiley Series in Probability and Statistics, 1998\n",
    "\n",
    "* Generalised Linear Models: Chapter 18\n",
    "* Logistic Regression: Section 18.3\n",
    "\n",
    "---\n",
    "\n",
    "## Example 1 - Regression with Categorical Variables\n",
    "\n",
    "For today's workshop we will need a few more packages and we run explicitely all the commands we need. Alternatively, one may outsource these into a separate file as we have done in the second workshop when we run the `%run setup.py` line (it allowed the lecturer to facilitate the interactive plots without confusing anyone with complicated lines of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836547bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import numpy package for numeric arrays and algebraic functions \n",
    "import numpy as np\n",
    "\n",
    "# Creating standard python plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating fancier and (arguably) prettier plots\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical packages\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as sk_lm\n",
    "from scipy import stats\n",
    "\n",
    "# Storing and manipulating data\n",
    "import pandas as pd\n",
    "\n",
    "# Some commands to make the plots look nice and big.\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['lines.markersize'] = 7\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "# Function returning the RMSE, you may recognise it from the last workshop\n",
    "def calc_RMSE(obs, fit, axis=1):\n",
    "    return np.sqrt(np.sum((obs-fit)**2)/len(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185e5c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Books Data\n",
    "\n",
    "This data set comes from the `DAAG` package in R and it gives the measurements on the volume (in cm$^3$) and weight (in g) of 15 books, some of which are softcover and some of which are hardback. Our goal with these data is to derive a model that allows us to predict the weight of a book give its volume and cover type.\n",
    "\n",
    "These data have been stored as variables in the `books.py` file provided with this worksheet, we will load the data using `import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbf0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from books import volume, weight, cover\n",
    "\n",
    "book_df = pd.DataFrame({'volume':volume, 'weight':weight, 'cover':cover}) \n",
    "\n",
    "n = len(volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b1e70",
   "metadata": {},
   "source": [
    "As with any new data the first order of business is to plot them and visually examine them for any potential relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot with datapoints coloured by cover-type\n",
    "sns.lmplot('volume', 'weight', data=book_df, hue='cover', fit_reg=False, palette=['b','r']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79cc41",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Based on the scatter plot above, how would you describe the relationship between the weight of the books and their volume? Does the cover type seem to play a role in this relationship?\n",
    "\n",
    "---\n",
    "\n",
    "We will now fit a linear regression model to these data and see what it tells us about their relationship(s). As in Example 2 of the last workshop, we will be using `sklearn`s `LinearRegression` function to fit the model and obtain our estimates for $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression object we will use to fit our models \n",
    "# Note that we have set fit_intercept to False, so we will need to \n",
    "# provide the column of ones if we want an intercept coefficient.\n",
    "\n",
    "linear_model = sk_lm.LinearRegression(fit_intercept=False)\n",
    "\n",
    "''' Construct the design matrix '''\n",
    "X = np.c_[np.ones(n), volume]     # Construct the model matrix by concatenating (putting together) an array of ones and a data array\n",
    "reg = linear_model.fit(X, weight) # Fit the linear model\n",
    "\n",
    "b0, b1 = reg.coef_                # Extract the coefficients (betas)\n",
    "\n",
    "print((b0,b1))\n",
    "\n",
    "''' Predict weight for observed volumes '''\n",
    "y_hat = reg.predict(X)\n",
    "\n",
    "\n",
    "''' Create a seaborn plot '''\n",
    "sns.lmplot('volume', 'weight', data=book_df, hue='cover', fit_reg=False, palette=['b','r'])  #Scatterplot with datapoints coloured by cover-type\n",
    "\n",
    "plt.plot(volume, y_hat, \"k-\") # Add regression line to scatter plot\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"$\\hat{y} = %.2f + %.2f x$    RMSE = %.2f\" % (b0, b1, calc_RMSE(weight, y_hat)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce04da",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "As mentioned in the lectures, a residual is the difference between the observed and predicted value, $y_i - \\hat{y}_i$. Create a scatter plot of the residuals vs fitted values, based on that plot what can you say about the residuals of the hardback books vs the softcover books? What does this observation tell us about potential weaknesses of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['fitted'] = y_hat\n",
    "book_df['residuals'] = book_df['weight'] - y_hat\n",
    "\n",
    "sns.lmplot('fitted', 'residuals', data=book_df, hue='cover', fit_reg=False, palette=['b','r'])  \n",
    "\n",
    "plt.axhline(y=0,ls=\"--\") \n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a48625",
   "metadata": {},
   "source": [
    "### Dummy Coding\n",
    "\n",
    "Based on these results, it should be clear that it is important that our model include information about whether or not a book is a hardback or softcover. As such, we need a way of encoding this information into our modeling framework. To do this we need a way of turning our text / categorical variable into a numeric representation that can be included in our model matrix.\n",
    "\n",
    "The most common approach for doing this is called dummy coding, in the case of a binary catergorical variable it involves picking one of the two levels of the categorical variable and encoding it as 1 and the other level as 0. With Python we can accomplish this by comparing our categorical vector to the value of our choice and then casting (converting) the result to an integer type.\n",
    "\n",
    "For example if we wanted to code `hardback` as 1 and `softcover` as 0 we would do the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardback = (cover == \"hardback\").astype(int) # Returns either 0 or 1\n",
    "print(hardback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfe022",
   "metadata": {},
   "source": [
    "This is equivalent to using an indicator function in mathematical notation,\n",
    "\n",
    "$$ \n",
    "\\mathbb{1}_{hb_i} = \n",
    "\\begin{cases}\n",
    "1 & \\text {if cover of book $i$ is hardback} \\\\\n",
    "0 & \\text {if cover of book $i$ is softcover}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Alternatively, we can defined the opposite of this where we code `hardback` as 0 and `softcover` as 1,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "softcover = (cover == \"softcover\").astype(int)\n",
    "print(softcover)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9510ba",
   "metadata": {},
   "source": [
    "Now that we have recoded our categorical variable, `cover`, into a numerical variable we can fit a standard regression model with the form\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, \\mathbb{1}_{hb_i}, $$\n",
    "\n",
    "which we can represent in matrix form using,\n",
    "$$\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$$  \n",
    "where \n",
    "$$\\boldsymbol{X} = \\big[ \\boldsymbol{1},\\, \\boldsymbol{x},\\, \\boldsymbol{\\mathbb{1}_{hb}} \\big].$$\n",
    "\n",
    "Using Python, we can use the usual concatenate function with our 1s column, the volume column, and our new dummy coded indicator column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(n), volume, hardback]\n",
    "reg = linear_model.fit(X, weight)\n",
    "\n",
    "b0, b1, b2 = reg.coef_\n",
    "print((b0,b1,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30edad",
   "metadata": {},
   "source": [
    "This gives us a regression equation of the form\n",
    "\n",
    "$$ \n",
    "y_i = 13.9 + 0.72 \\, x_i + 184.0 \\, \\mathbb{1}_{hb_i} ,\n",
    "$$\n",
    "\n",
    "which can be rewritten as two separate line equations (one for each case of `cover`)\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "        13.9 + 0.72 \\, x_i & \\text{if book cover $i$ is hardback} \\\\\n",
    "        (13.9 + 184.0) + 0.72 \\, x_i & \\text{if book cover $i$ is softcover} \\\\\n",
    "       \\end{cases}.\n",
    "$$\n",
    "\n",
    "We can calculate prediction points along those lines using the following Python code in which we hard code the possible values of $\\boldsymbol{\\mathbb{1}_{hb_i}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ba35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg.predict(X)              # Predict weights for the observed volumes\n",
    "\n",
    "sc = b0 + b1 * volume + b2 * 0 # prediction volumes for softcover books\n",
    "hb = b0 + b1 * volume + b2 * 1 # prediction volumes for hardback books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69f6d0",
   "metadata": {},
   "source": [
    "We can then plot both of these lines along with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf23aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('volume', 'weight', data=book_df, hue='cover', fit_reg=False, palette=['b','r'])\n",
    "plt.plot(volume, hb, 'b-')  # Add hardback regression line\n",
    "plt.plot(volume, sc, 'r-') # Add softcover regression line\n",
    "plt.title(\"(RMSE = %.2f)\" % calc_RMSE(weight, y_hat))\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a79212",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Based on these regression fits, do you think the model including the dummy coded `cover` variable produces a \"better\" model than our first regression model which did not include `cover`? Explain. \n",
    "\n",
    "---\n",
    "\n",
    "Note that by including a dummy variable in our model we must also change the interpretation of our regression coefficients. In this context,\n",
    "\n",
    "* $\\beta_0$ - This is the expected weight of a book with a `volume` of zero and a `hardback` indicator of zero, in other words a softcover book with zero volume.\n",
    "\n",
    "* $\\beta_1$ - This is the expected additional weight a book would have if its volume were to increase by 1 cm$^3$, all else being equal.\n",
    "\n",
    "* $\\beta_2$ - This is the expected additional weight a book would have if its hardcover indicator were to increase by 1, all else being equal. However, the hardcover indicator can only be 0 or 1 and hence this is the change in weight we would expect between a softcover book and a hardcover book with the same volume. In other words, hardcover books weight 184g more than softcover books.\n",
    "\n",
    "Based on these interpretations we can see that the level that was coded as 0 (what is often called the reference level) gets folded into our intercept. The slope coefficient for the indicator provides the difference in intercept between the reference and the contrast level (level coded as 1).\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Repeat the analysis above but this time fit a model using `softcover` instead of `hardcover` in your design matrix. You should fit the new model as well as calculate the predictions for both softcover and hardback books. Finally, create a scatter plot of the data along with the model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47655765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adcc5f",
   "metadata": {},
   "source": [
    "### &diams; Exercise &diams;\n",
    "\n",
    "What changed between the model with `softcover` vs the model with `hardcover`? Specifically, comment on the values of $\\beta_0$, $\\beta_1$, and $\\beta_2$ and their interpretations.\n",
    "\n",
    "---\n",
    "\n",
    "Now lets consider the model where we naievely include both `hardback` and `softcover` as predictors in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(n), volume, hardback, softcover]\n",
    "reg = linear_model.fit(X, weight)\n",
    "\n",
    "b0, b1, b2, b3 = reg.coef_\n",
    "print( (b0, b1, b2, b3) )\n",
    "\n",
    "\n",
    "y_hat = reg.predict(X)\n",
    "\n",
    "hb = b0 + b1 * volume + b2 * 1 + b3 * 0\n",
    "sc = b0 + b1 * volume + b2 * 0 + b3 * 1\n",
    "\n",
    "sns.lmplot('volume', 'weight', data=book_df, hue='cover', fit_reg=False, palette=['b','r'])\n",
    "plt.plot(volume, hb, 'b-')  # Add hardback regression line\n",
    "plt.plot(volume, sc, 'r-') # Add softcover regression line\n",
    "plt.title(\"(RMSE = %.2f)\" % calc_RMSE(weight, y_hat))\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb88ad1",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Write out the equations that predict weight for hardback and softcover books according to this model.  \n",
    "\n",
    "---\n",
    "\n",
    "### Question \n",
    "\n",
    "Are the solutions given above unique? Can you find different values of $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ that would give you the same regression equations you wrote out in the exercise earlier (marked with a diamond &diams;)?\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2\n",
    "\n",
    "This is an example with real data, taken from *Davison*. The aim of the experiment conducted was to see if butterflies have better protection from being eaten if their wings are marked with different colours. Data was collected for six different species and eight different colours. It was then checked if the butterflies were either \n",
    "* not sampled by the predator at all,\n",
    "* sampled by the predator but not eaten, or\n",
    "* eaten. \n",
    "\n",
    "We firstly load the data stored in separate files, one for each species. We print the first species and all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e90521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "''' Load data for each butterfly species '''\n",
    "aphrissa_boisduvalli = np.load('rm_aphrissa_boisduvalli.npy')\n",
    "phoebis_argante = np.load('rm_phoebis_argante.npy')\n",
    "dryas_iulia = np.load('rm_dryas_iulia.npy')\n",
    "pierella_luna = np.load('rm_pierella_luna.npy')\n",
    "consul_fabius = np.load('rm_consul_fabius.npy')\n",
    "siproeta_stelenes = np.load('rm_siproeta_stelenes.npy')\n",
    "\n",
    "print('Aphrissa Boisduvalli: ',aphrissa_boisduvalli)\n",
    "print('\\nAll species:: ',np.c_[aphrissa_boisduvalli,phoebis_argante,dryas_iulia,pierella_luna,consul_fabius,siproeta_stelenes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9caf82c",
   "metadata": {},
   "source": [
    "We will be interested in whether the colours increase the *probability* of the species being eaten, and aggregate all the different butterfly species to one. We then sum the first two columns, as both the 'not sampled' and the 'sampled and rejected' columns correspond to 'not eaten'.\n",
    "\n",
    "The data is then printed together with the row labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Aggregate data '''\n",
    "all_butterflies = aphrissa_boisduvalli+phoebis_argante+dryas_iulia+pierella_luna+consul_fabius+siproeta_stelenes\n",
    "\n",
    "''' Sum first the first two rows (corresponding to \"N\" and \"S\") to get the \"Not eaten\" column '''\n",
    "not_eaten = all_butterflies[:,0]+all_butterflies[:,1]\n",
    "\n",
    "''' \"Eaten column\" is the third column of the all_butterflies matrix '''\n",
    "eaten = all_butterflies[:,2]\n",
    "\n",
    "data = np.c_[not_eaten,eaten]\n",
    "data = data.transpose()\n",
    "\n",
    "print('             NE  E')\n",
    "print('Unpainted: ',data[:,0])\n",
    "print('Brown:     ',data[:,1])\n",
    "print('Yellow:    ',data[:,2])\n",
    "print('Blue:      ',data[:,3])\n",
    "print('Green:     ',data[:,4])\n",
    "print('Red:       ',data[:,5])\n",
    "print('Orange:    ',data[:,6])\n",
    "print('Black:     ',data[:,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f48f5",
   "metadata": {},
   "source": [
    "We are interested in the probability of being eaten, so need to transpose the data. We initialise the response and the predictor matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2337b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()\n",
    "print(data)\n",
    "\n",
    "y = np.zeros(8) # 8 colours\n",
    "X = np.eye(8) # dummy variables\n",
    "for i in range(8):\n",
    "    y[i] = data[1,i]/(data[0,i]+data[1,i]) # probability eaten\n",
    "    \n",
    "print('\\nResponses: ',y)\n",
    "print('\\nDesign matrix: ',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87976fe",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "If one is very precise, we would have a data point for each butterfly. If e.g. the $i$-th bubtterfly is blue and eaten, the butterfly would have the response $y_i=1$ and the predictor $(0,0,0,1,0,0,0,0)$. Check that the response vector and design matrix above result in the same estimate $\\hat\\beta$.\n",
    "\n",
    "---\n",
    "We now run the file 'IRLS.py' which contains an iterative solver to get a least squares estimate for the logistic regression problem.\n",
    "\n",
    "### Exercise\n",
    "Check the file 'IRLS.py'. The iterative solver starts at a given point $\\beta^{(0)}$, and iteratively updates this estimate $\\beta^{(k)}$. Write down the mathematical expression for $\\beta^{(k)}$. When does the iterative algorithm stop?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f98911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run IRLS.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c1400",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Choose different start vectors and find the least squares estimates using `beta_hat = IRLS(beta_start,X,y)`. Do the results change depending on the start points chosen?\n",
    "\n",
    "Print the responses $y$ and the transformed probabilities $\\mu_i = \\frac{1}{1+\\exp(-X_{i,:}^T\\beta^{(k)})}$. What can you say about the resulting estimate $\\hat\\beta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start1 = np.ones(8)\n",
    "beta_hat_LL1 = IRLS(beta_start1,X,y)\n",
    "print('beta_hat_LL1 = ',beta_hat_LL1)\n",
    "\n",
    "beta_start2 = # INSERT CODE\n",
    "beta_hat_LL2 = IRLS(beta_start2,X,y)\n",
    "print('beta_hat_LL2 = ',beta_hat_LL2)\n",
    "\n",
    "beta_start3 = # INSERT CODE\n",
    "# INSERT CODE\n",
    "\n",
    "beta_start4 = # INSERT CODE\n",
    "# INSERT CODE\n",
    "\n",
    "\n",
    "''' Let us also look at the transformed estimates (i.e. the probabilities) '''\n",
    "probs = np.zeros(8)\n",
    "for i in range(8):\n",
    "    probs[i] = 1/(1+np.exp(-np.dot(beta_hat_LL,X[i,:])))\n",
    "print('\\nProbabilities = ',probs)\n",
    "\n",
    "''' Compare this to the input values '''\n",
    "print('Reponses = ',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27906cf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Example 3\n",
    "Let us now consider a logistic regression example with continuous predictors. Load the predictors stored in the file 'rm3_ex4_x.npy', and the responses from  'rm3_ex4_y.npy'. The data describes how likely students are to pass an exam, depending on how many hours they spend per week listening to the lectures (variable $x_1$) and how many hours they spend on the workshops (variable $x_2$). \n",
    "\n",
    "### Exercise\n",
    "Create a figure with two subplots, one plotting the responses against the first predictor, the second one the reponses against the second predictor. What correlations can you see in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f02b1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The next step is the regression analysis. To this end, define the design matrix with an intercept, and the two variables. Run the line `%run IRLS.py` which allows you to call the iteratively reweighted least squqres (IRLS) algorithm using the lines\n",
    "`beta_hat = IRLS(beta_start,X,y)` with an appropriate start vector $\\beta_{start}$. Print the resulting estimates for different values of $\\beta_{start}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ed0cf",
   "metadata": {},
   "source": [
    "Note that, depending on the choice of $\\beta_{start}$, we get different estimates for $\\hat\\beta$. \n",
    "\n",
    "### Exercise \n",
    "To see which estimates are sensible, transform the estimates in probabilities through $\\hat y_i = \\frac{1}{1+\\exp(-X_{i,:}^T\\hat\\beta)}$ for the respective estimates $\\hat\\beta$, and compare them to the given data points by computing the RMSE $\\sqrt{\\frac1n\\sum_{i=1}^n(y_i-\\hat y_i)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef300767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
